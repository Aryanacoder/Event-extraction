{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (c:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1344\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:84\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlignDevicesHook, add_hook_to_module\n\u001b[32m     87\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGreedySearchDecoderOnlyOutput\u001b[39;00m(ModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\__init__.py:16\u001b[39m\n\u001b[32m     14\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m1.6.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     cpu_offload,\n\u001b[32m     19\u001b[39m     cpu_offload_with_hook,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     load_checkpoint_and_dispatch,\n\u001b[32m     25\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:34\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhooks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_torch_state_dict_into_shards\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_torchao_available\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (c:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1344\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:42\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[32m     33\u001b[39m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     TokenClassifierOutput,\n\u001b[32m     41\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:41\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig, GenerationMixin\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1335\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1336\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1348\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1350\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (c:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbackoff\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForTokenClassification, pipeline\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline \u001b[38;5;28;01mas\u001b[39;00m transformers_pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1336\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   1335\u001b[39m     module = \u001b[38;5;28mself\u001b[39m._get_module(\u001b[38;5;28mself\u001b[39m._class_to_module[name])\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     value = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1335\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1333\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._get_module(name)\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1336\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1348\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1350\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (c:\\Users\\Aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "import backoff\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from transformers import pipeline as transformers_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import schedule\n",
    "import json\n",
    "import hdbscan\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import warnings\n",
    "import tweepy\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Failed to download NLTK data: {e}. Sentiment analysis may not work properly.\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 16:09:12,715 - __main__ - INFO - Loaded spaCy model\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "2025-04-04 16:09:14,154 - __main__ - INFO - Loaded BERT NER model\n",
      "Device set to use cpu\n",
      "2025-04-04 16:09:15,981 - __main__ - INFO - Loaded advanced NER model\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define search queries for disaster types\n",
    "disaster_queries = [\n",
    "    # Natural Disasters\n",
    "    \"earthquake\",\n",
    "    \"flood\",\n",
    "    \"hurricane\",\n",
    "    \"tornado\",\n",
    "    \"tsunami\",\n",
    "    \"wildfire\",\n",
    "    \"drought\",\n",
    "    \"landslide\",\n",
    "    \"volcanic eruption\",\n",
    "    \"extreme weather\",\n",
    "    # Non-Natural Disasters\n",
    "    \"industrial accident\",\n",
    "    \"chemical spill\",\n",
    "    \"radiation leak\",\n",
    "    \"explosion\",\n",
    "    \"airplane crash\",\n",
    "    \"train derailment\",\n",
    "    \"shipwreck\",\n",
    "    \"road accident\",\n",
    "    \"structural collapse\",\n",
    "    \"mine accident\",\n",
    "    \"oil spill\",\n",
    "    \"war\",\n",
    "    \"civil war\",\n",
    "    \"ethnic conflict\",\n",
    "    \"terrorist attack\",\n",
    "    \"bombing\",\n",
    "    \"hostage situation\",\n",
    "    \"cyber attack\",\n",
    "    \"urban fire\",\n",
    "    \"industrial fire\"\n",
    "]\n",
    "\n",
    "# Load the spaCy model\n",
    "try:\n",
    "    nlp_spacy = spacy.load('en_core_web_sm')\n",
    "    logger.info(\"Loaded spaCy model\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load spaCy model: {e}\")\n",
    "    logger.info(\"Installing spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model for NER\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "    model = BertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "    nlp_ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "    logger.info(\"Loaded BERT NER model\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load BERT NER model: {e}\")\n",
    "    nlp_ner = None\n",
    "\n",
    "# Load more advanced NER model\n",
    "try:\n",
    "    advanced_ner_tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "    advanced_ner_model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "    advanced_nlp_ner = transformers_pipeline('ner', model=advanced_ner_model, tokenizer=advanced_ner_tokenizer, aggregation_strategy=\"simple\")\n",
    "    logger.info(\"Loaded advanced NER model\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load advanced NER model: {e}\")\n",
    "    advanced_nlp_ner = None\n",
    "\n",
    "# Load disaster classification model\n",
    "try:\n",
    "    disaster_classifier_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "    disaster_classifier_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "    disaster_classifier = transformers_pipeline(\"zero-shot-classification\", model=disaster_classifier_model, tokenizer=disaster_classifier_tokenizer)\n",
    "    logger.info(\"Loaded disaster classification model\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load disaster classification model: {e}\")\n",
    "    disaster_classifier = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_twitter_api():\n",
    "    \"\"\"Set up Twitter API connection using tweepy\"\"\"\n",
    "    try:\n",
    "    \n",
    "        consumer_key = \"Dh7fqMnZ5jwjnNfAWkyHQZ4oZ\"\n",
    "        consumer_secret = \"ie5wbRumiTZCrFZaFKulJkIhkYz8ISqWhEjPCorbcBElE66rwY\"\n",
    "        access_token = \"1222955679754752000-ovmXhFKfsjE31IETm4Rle2xyF6n78m\"\n",
    "        access_token_secret = \"leSB6TlOtppIwgAtFOPGMNsnMZLkKIEbH7SYlYantsGGM\"\n",
    "        bearer_token = \"AAAAAAAAAAAAAAAAAAAAAJeJ0QEAAAAA96O7kdxKzyJCN4EJquDBjsutMvo%3D9GIgE3np6a1Y2j0mY4h5sSXfWWSVtWjckuhua9c1zQbi5EX24t\"\n",
    "\n",
    "\n",
    "        \n",
    "        # Authentication with Twitter\n",
    "        auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        \n",
    "        # Test the connection\n",
    "        api.verify_credentials()\n",
    "        logger.info(\"Twitter API connection established successfully\")\n",
    "        return api\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Twitter API: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_twitter_data(api, query, hours_ago=6, max_tweets=100):\n",
    "    \"\"\"Fetch tweets related to disaster query from the past few hours\"\"\"\n",
    "    if not api:\n",
    "        logger.error(\"Twitter API not initialized\")\n",
    "        return []\n",
    "    \n",
    "    tweets = []\n",
    "    try:\n",
    "        # Calculate time range\n",
    "        current_time = datetime.utcnow()\n",
    "        time_ago = current_time - timedelta(hours=hours_ago)\n",
    "        \n",
    "        # Format date for Twitter search\n",
    "        date_since = time_ago.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Search query with disaster term and filter for English tweets\n",
    "        search_query = f\"{query} -filter:retweets lang:en\"\n",
    "        \n",
    "        # Fetch tweets\n",
    "        fetched_tweets = tweepy.Cursor(\n",
    "            api.search_tweets,\n",
    "            q=search_query,\n",
    "            lang=\"en\",\n",
    "            since_id=date_since,\n",
    "            tweet_mode=\"extended\"\n",
    "        ).items(max_tweets)\n",
    "        \n",
    "        # Process tweets\n",
    "        for tweet in fetched_tweets:\n",
    "            # Check if tweet is within time range\n",
    "            tweet_time = tweet.created_at\n",
    "            if tweet_time >= time_ago:\n",
    "                tweet_data = {\n",
    "                    'id': tweet.id_str,\n",
    "                    'text': tweet.full_text if hasattr(tweet, 'full_text') else tweet.text,\n",
    "                    'created_at': tweet_time,\n",
    "                    'user': tweet.user.screen_name,\n",
    "                    'location': tweet.user.location,\n",
    "                    'coordinates': tweet.coordinates,\n",
    "                    'place': tweet.place.full_name if tweet.place else None,\n",
    "                    'retweet_count': tweet.retweet_count,\n",
    "                    'favorite_count': tweet.favorite_count,\n",
    "                    'query': query\n",
    "                }\n",
    "                tweets.append(tweet_data)\n",
    "        \n",
    "        logger.info(f\"Fetched {len(tweets)} tweets for query: {query}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching tweets for query {query}: {e}\")\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "def process_twitter_data(tweets):\n",
    "    \"\"\"Process tweets to extract disaster information\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            # Extract text\n",
    "            text = tweet['text']\n",
    "            \n",
    "            # Extract locations\n",
    "            locations = []\n",
    "            if tweet['place']:\n",
    "                locations.append(tweet['place'])\n",
    "            if tweet['location']:\n",
    "                locations.append(tweet['location'])\n",
    "            \n",
    "            # If coordinates are directly available, use them\n",
    "            coordinates = None\n",
    "            if tweet['coordinates'] and tweet['coordinates']['coordinates']:\n",
    "                # Twitter uses [longitude, latitude] format\n",
    "                lon, lat = tweet['coordinates']['coordinates']\n",
    "                coordinates = (lat, lon)\n",
    "            \n",
    "            # If no coordinates but locations found, geocode them\n",
    "            if not coordinates and locations:\n",
    "                for location in locations:\n",
    "                    coords = location_to_coordinates(location)\n",
    "                    if coords:\n",
    "                        coordinates = coords\n",
    "                        break\n",
    "            \n",
    "            # Extract impact variables\n",
    "            deaths, injured, affected, homeless = extract_impact_variables(text)\n",
    "            \n",
    "            # Calculate severity score\n",
    "            severity_score = continuous_severity_classification(deaths, injured, affected, homeless)\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            sentiment = analyze_sentiment(text)\n",
    "            \n",
    "            # Store the disaster type\n",
    "            disaster_type = tweet['query']\n",
    "            \n",
    "            # Create event data\n",
    "            if coordinates:\n",
    "                event_data = {\n",
    "                    'source': 'twitter',\n",
    "                    'disaster_type': disaster_type,\n",
    "                    'location': locations[0] if locations else \"Unknown\",\n",
    "                    'latitude': coordinates[0],\n",
    "                    'longitude': coordinates[1],\n",
    "                    'date': tweet['created_at'].strftime(\"%Y-%m-%d\"),\n",
    "                    'time': tweet['created_at'].strftime(\"%H:%M:%S\"),\n",
    "                    'severity_score': severity_score,\n",
    "                    'deaths': deaths,\n",
    "                    'injured': injured,\n",
    "                    'affected': affected,\n",
    "                    'homeless': homeless,\n",
    "                    'sentiment': sentiment['compound'],\n",
    "                    'url': f\"https://twitter.com/{tweet['user']}/status/{tweet['id']}\",\n",
    "                    'text': text,\n",
    "                    'retweet_count': tweet['retweet_count'],\n",
    "                    'favorite_count': tweet['favorite_count']\n",
    "                }\n",
    "                processed_data.append(event_data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing tweet {tweet.get('id', 'unknown')}: {e}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def analyze_twitter_trends(twitter_data):\n",
    "    \"\"\"Analyze trends in Twitter data related to disasters\"\"\"\n",
    "    if not twitter_data:\n",
    "        logger.warning(\"No Twitter data to analyze\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame if not already\n",
    "    if not isinstance(twitter_data, pd.DataFrame):\n",
    "        df = pd.DataFrame(twitter_data)\n",
    "    else:\n",
    "        df = twitter_data.copy()\n",
    "    \n",
    "    # Calculate engagement metrics\n",
    "    df['engagement'] = df['retweet_count'] + df['favorite_count']\n",
    "    \n",
    "    # Find most engaging tweets\n",
    "    top_tweets = df.sort_values('engagement', ascending=False).head(10)\n",
    "    \n",
    "    # Analyze hashtags\n",
    "    hashtags = []\n",
    "    hashtag_pattern = r'#(\\w+)'\n",
    "    for text in df['text']:\n",
    "        matches = re.findall(hashtag_pattern, text)\n",
    "        hashtags.extend([tag.lower() for tag in matches])\n",
    "    \n",
    "    hashtag_counts = Counter(hashtags)\n",
    "    top_hashtags = hashtag_counts.most_common(20)\n",
    "    \n",
    "    # Analyze mentions\n",
    "    mentions = []\n",
    "    mention_pattern = r'@(\\w+)'\n",
    "    for text in df['text']:\n",
    "        matches = re.findall(mention_pattern, text)\n",
    "        mentions.extend([mention.lower() for mention in matches])\n",
    "    \n",
    "    mention_counts = Counter(mentions)\n",
    "    top_mentions = mention_counts.most_common(10)\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot top hashtags\n",
    "    plt.subplot(2, 1, 1)\n",
    "    hashtag_df = pd.DataFrame(top_hashtags, columns=['Hashtag', 'Count'])\n",
    "    sns.barplot(x='Count', y='Hashtag', data=hashtag_df)\n",
    "    plt.title('Top Hashtags in Disaster Tweets')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Plot engagement by disaster type\n",
    "    plt.subplot(2, 1, 2)\n",
    "    engagement_by_type = df.groupby('disaster_type')['engagement'].mean().sort_values(ascending=False)\n",
    "    engagement_by_type.plot(kind='bar')\n",
    "    plt.title('Average Engagement by Disaster Type')\n",
    "    plt.ylabel('Average Engagement (Retweets + Favorites)')\n",
    "    plt.xlabel('Disaster Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    viz_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"twitter_analysis.png\")\n",
    "    plt.savefig(viz_file)\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Twitter analysis visualization saved to {viz_file}\")\n",
    "    \n",
    "    # Create report\n",
    "    report = \"# Twitter Disaster Data Analysis\\n\\n\"\n",
    "    report += f\"Analysis generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "    \n",
    "    report += \"## Summary\\n\\n\"\n",
    "    report += f\"Total tweets analyzed: {len(df)}\\n\"\n",
    "    report += f\"Disaster types mentioned: {df['disaster_type'].nunique()}\\n\"\n",
    "    report += f\"Average engagement per tweet: {df['engagement'].mean():.2f}\\n\\n\"\n",
    "    \n",
    "    report += \"## Top Hashtags\\n\\n\"\n",
    "    for hashtag, count in top_hashtags:\n",
    "        report += f\"- #{hashtag}: {count}\\n\"\n",
    "    \n",
    "    report += \"\\n## Top Mentions\\n\\n\"\n",
    "    for mention, count in top_mentions:\n",
    "        report += f\"- @{mention}: {count}\\n\"\n",
    "    \n",
    "    report += \"\\n## Most Engaging Tweets\\n\\n\"\n",
    "    for i, (_, tweet) in enumerate(top_tweets.iterrows()):\n",
    "        report += f\"{i+1}. **{tweet['disaster_type']}** (Engagement: {tweet['engagement']})\\n\"\n",
    "        report += f\"   {tweet['text'][:100]}...\\n\"\n",
    "        report += f\"   [View Tweet](https://twitter.com/{tweet['user']}/status/{tweet['id']})\\n\\n\"\n",
    "    \n",
    "    # Save report\n",
    "    report_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"twitter_analysis.md\")\n",
    "    try:\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        logger.info(f\"Twitter analysis report saved to {report_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save Twitter analysis report: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'top_hashtags': top_hashtags,\n",
    "        'top_mentions': top_mentions,\n",
    "        'top_tweets': top_tweets.to_dict('records')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backoff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@backoff\u001b[39m.on_exception(backoff.expo, (requests.exceptions.RequestException, requests.exceptions.HTTPError), max_tries=\u001b[32m5\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_article_content\u001b[39m(url):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fetch article content from URL with error handling and backoff\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'backoff' is not defined"
     ]
    }
   ],
   "source": [
    "@backoff.on_exception(backoff.expo, (requests.exceptions.RequestException, requests.exceptions.HTTPError), max_tries=5)\n",
    "def fetch_article_content(url):\n",
    "    \"\"\"Fetch article content from URL with error handling and backoff\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, timeout=10, headers=headers)  # Set a timeout to avoid hanging\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        return response.content\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {url}\")\n",
    "        raise\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        logger.error(f\"Connection error occurred: {conn_err} - {url}\")\n",
    "        raise\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        logger.error(f\"Timeout error occurred: {timeout_err} - {url}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        logger.error(f\"Request error occurred: {req_err} - {url}\")\n",
    "        raise\n",
    "\n",
    "def parse_article_content(html_content):\n",
    "    \"\"\"Parse HTML content to extract article text\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Try to find the main article content\n",
    "    article = soup.find('article')\n",
    "    if article:\n",
    "        paragraphs = article.find_all('p')\n",
    "    else:\n",
    "        # Try common content containers\n",
    "        content_div = soup.find('div', class_=['article-content', 'story-body', 'main-content', 'article-body', 'entry-content'])\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "        else:\n",
    "            # Fallback to all paragraphs\n",
    "            paragraphs = soup.find_all('p')\n",
    "    \n",
    "    article_text = ' '.join([para.text for para in paragraphs])\n",
    "    \n",
    "    # Clean the text\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)  # Remove extra whitespace\n",
    "    article_text = re.sub(r'[^\\x00-\\x7F]+', '', article_text)  # Remove non-ASCII characters\n",
    "    \n",
    "    return article_text\n",
    "\n",
    "def extract_ner_entities(text, nlp):\n",
    "    \"\"\"Extract named entities from text using NER model\"\"\"\n",
    "    # Break text into chunks if it's too long\n",
    "    max_length = 512\n",
    "    chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    \n",
    "    all_entities = {}\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            ner_results = nlp(chunk)\n",
    "            for entity in ner_results:\n",
    "                entity_parts = entity['entity'].split('-')\n",
    "                if len(entity_parts) > 1:\n",
    "                    entity_type = entity_parts[1]  # Get the entity type\n",
    "                    entity_text = entity['word']\n",
    "                    if entity_type not in all_entities:\n",
    "                        all_entities[entity_type] = []\n",
    "                    all_entities[entity_type].append(entity_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in NER processing: {e}\")\n",
    "    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_locations(text):\n",
    "    \"\"\"Extract location entities from text using NER models\"\"\"\n",
    "    # Try with advanced NER model first if available\n",
    "    if advanced_nlp_ner:\n",
    "        try:\n",
    "            entities = advanced_nlp_ner(text)\n",
    "            locations = [entity['word'] for entity in entities if entity['entity_group'] in ['LOC', 'GPE']]\n",
    "            if locations:\n",
    "                return locations\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Advanced NER failed: {e}. Falling back to spaCy.\")\n",
    "    \n",
    "    # Fallback to spaCy\n",
    "    doc = nlp_spacy(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    return [loc for loc in locations if not (loc in seen or seen.add(loc))]\n",
    "\n",
    "@backoff.on_exception(backoff.expo, (GeocoderTimedOut, GeocoderServiceError), max_tries=5)\n",
    "def location_to_coordinates(location):\n",
    "    \"\"\"Convert location name to geographic coordinates\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"disaster_event_extraction\")\n",
    "    try:\n",
    "        location_data = geolocator.geocode(location)\n",
    "        if location_data:\n",
    "            return (location_data.latitude, location_data.longitude)\n",
    "        else:\n",
    "            logger.warning(f\"Location not found: {location}\")\n",
    "            return None\n",
    "    except GeocoderTimedOut:\n",
    "        logger.warning(f\"Geocoding service timed out for location: {location}\")\n",
    "        raise\n",
    "    except GeocoderServiceError as e:\n",
    "        logger.warning(f\"Geocoding service error for location {location}: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error geocoding location {location}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_dates(text):\n",
    "    \"\"\"Extract date entities from text\"\"\"\n",
    "    doc = nlp_spacy(text)\n",
    "    dates = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "    \n",
    "    # Try to parse dates to a standard format\n",
    "    parsed_dates = []\n",
    "    for date_str in dates:\n",
    "        try:\n",
    "            # This is a simple approach - in production, use dateparser library\n",
    "            date_formats = [\"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\", \"%B %d, %Y\", \"%d %B %Y\"]\n",
    "            for fmt in date_formats:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, fmt)\n",
    "                    parsed_dates.append(date_obj)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Failed to parse date '{date_str}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    return parsed_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_impact_variables(article_text):\n",
    "    \"\"\"Extract impact variables like deaths, injuries from text\"\"\"\n",
    "    # Initialize variables\n",
    "    deaths = 0\n",
    "    injured = 0\n",
    "    affected = 0\n",
    "    homeless = 0\n",
    "    \n",
    "    # First try with NER if available\n",
    "    if nlp_ner:\n",
    "        entities = extract_ner_entities(article_text, nlp_ner)\n",
    "        \n",
    "        # Try to extract using NER entities\n",
    "        if 'NUM' in entities:\n",
    "            for num in entities['NUM']:\n",
    "                try:\n",
    "                    num_value = int(num)\n",
    "                    if re.search(r'(death|dead|killed|fatalities)', article_text.lower()):\n",
    "                        deaths += num_value\n",
    "                    elif re.search(r'(injured|wounded|hurt)', article_text.lower()):\n",
    "                        injured += num_value\n",
    "                    elif re.search(r'(affected|impacted|hit)', article_text.lower()):\n",
    "                        affected += num_value\n",
    "                    elif re.search(r'(homeless|displaced|evacuated)', article_text.lower()):\n",
    "                        homeless += num_value\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    # Use regex patterns as a backup or enhancement\n",
    "    death_patterns = [\n",
    "        r'(\\d+)\\s+(?:people|person|individuals?)?\\s+(?:were|was|have been|has been)?\\s+killed',\n",
    "        r'death\\s+toll\\s+(?:of|is|has reached|reached|climbed to)?\\s+(\\d+)',\n",
    "        r'(\\d+)\\s+deaths',\n",
    "        r'(\\d+)\\s+dead'\n",
    "    ]\n",
    "    \n",
    "    injured_patterns = [\n",
    "        r'(\\d+)\\s+(?:people|person|individuals?)?\\s+(?:were|was|have been|has been)?\\s+injured',\n",
    "        r'(\\d+)\\s+injured',\n",
    "        r'(\\d+)\\s+wounded'\n",
    "    ]\n",
    "    \n",
    "    affected_patterns = [\n",
    "        r'(\\d+)\\s+(?:people|person|individuals?)?\\s+(?:were|was|have been|has been)?\\s+affected',\n",
    "        r'affecting\\s+(\\d+)\\s+people',\n",
    "        r'(\\d+)\\s+affected'\n",
    "    ]\n",
    "    \n",
    "    homeless_patterns = [\n",
    "        r'(\\d+)\\s+(?:people|person|individuals?)?\\s+(?:were|was|have been|has been)?\\s+(?:left)?\\s+homeless',\n",
    "        r'(\\d+)\\s+homeless',\n",
    "        r'displaced\\s+(\\d+)\\s+people'\n",
    "    ]\n",
    "    \n",
    "    # Extract numbers using regex patterns\n",
    "    for pattern in death_patterns:\n",
    "        matches = re.findall(pattern, article_text.lower())\n",
    "        for match in matches:\n",
    "            try:\n",
    "                deaths += int(match)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    for pattern in injured_patterns:\n",
    "        matches = re.findall(pattern, article_text.lower())\n",
    "        for match in matches:\n",
    "            try:\n",
    "                injured += int(match)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    for pattern in affected_patterns:\n",
    "        matches = re.findall(pattern, article_text.lower())\n",
    "        for match in matches:\n",
    "            try:\n",
    "                affected += int(match)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    for pattern in homeless_patterns:\n",
    "        matches = re.findall(pattern, article_text.lower())\n",
    "        for match in matches:\n",
    "            try:\n",
    "                homeless += int(match)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    return deaths, injured, affected, homeless\n",
    "\n",
    "def continuous_severity_classification(deaths, injured, affected, homeless, infrastructure_damage=0, economic_loss=0):\n",
    "    \"\"\"Calculate severity score based on impact variables\"\"\"\n",
    "    # Weights for different factors\n",
    "    weights = {\n",
    "        'deaths': 0.35,\n",
    "        'injured': 0.20,\n",
    "        'affected': 0.15,\n",
    "        'homeless': 0.15,\n",
    "        'infrastructure': 0.10,\n",
    "        'economic': 0.05\n",
    "    }\n",
    "    \n",
    "    # Apply log transformation to handle wide ranges of values\n",
    "    severity_score = (\n",
    "        weights['deaths'] * math.log1p(deaths) +\n",
    "        weights['injured'] * math.log1p(injured) +\n",
    "        weights['affected'] * math.log1p(affected) +\n",
    "        weights['homeless'] * math.log1p(homeless) +\n",
    "        weights['infrastructure'] * math.log1p(infrastructure_damage) +\n",
    "        weights['economic'] * math.log1p(economic_loss)\n",
    "    )\n",
    "    \n",
    "    return severity_score\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment of text using VADER\"\"\"\n",
    "    try:\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        sentiment = sia.polarity_scores(text)\n",
    "        return sentiment\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sentiment analysis failed: {e}\")\n",
    "        return {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}\n",
    "\n",
    "def classify_disaster_type(text):\n",
    "    \"\"\"Classify disaster type using zero-shot classification\"\"\"\n",
    "    if not disaster_classifier:\n",
    "        return None, 0.0\n",
    "    \n",
    "    try:\n",
    "        result = disaster_classifier(\n",
    "            text, \n",
    "            candidate_labels=disaster_queries,\n",
    "            multi_label=False\n",
    "        )\n",
    "        return result['labels'][0], result['scores'][0]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in disaster classification: {e}\")\n",
    "        return None, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(locations, severity_scores, disaster_types):\n",
    "    \"\"\"Create an interactive map with disaster locations\"\"\"\n",
    "    # Create a map centered at the first valid location or default to world center\n",
    "    valid_locations = [(lat, lon) for lat, lon in locations if isinstance(lat, (int, float)) and isinstance(lon, (int, float))]\n",
    "    if valid_locations:\n",
    "        map_center = valid_locations[0]\n",
    "    else:\n",
    "        map_center = [0, 0]\n",
    "    \n",
    "    m = folium.Map(location=map_center, zoom_start=2, tiles=\"OpenStreetMap\")\n",
    "    \n",
    "    # Add a marker cluster\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    \n",
    "    # Normalize severity scores for color intensity\n",
    "    if severity_scores:\n",
    "        max_severity = max(severity_scores)\n",
    "        min_severity = min(severity_scores)\n",
    "        severity_range = max_severity - min_severity if max_severity != min_severity else 1\n",
    "    \n",
    "    # Add markers for each location\n",
    "    for i, (lat, lon) in enumerate(locations):\n",
    "        if isinstance(lat, (int, float)) and isinstance(lon, (int, float)):\n",
    "            # Calculate color based on severity (red for high severity)\n",
    "            if severity_scores:\n",
    "                normalized_severity = (severity_scores[i] - min_severity) / severity_range\n",
    "                color = f'#{int(255 * normalized_severity):02x}0000'  # Red with intensity based on severity\n",
    "            else:\n",
    "                color = 'red'\n",
    "            \n",
    "            # Create popup content\n",
    "            popup_content = f\"\"\"\n",
    "            <b>Disaster Type:</b> {disaster_types[i]}<br>\n",
    "            <b>Severity Score:</b> {severity_scores[i]:.2f}<br>\n",
    "            <b>Coordinates:</b> {lat:.4f}, {lon:.4f}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Add marker\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                popup=folium.Popup(popup_content, max_width=300),\n",
    "                icon=folium.Icon(color='red', icon='info-sign')\n",
    "            ).add_to(marker_cluster)\n",
    "    \n",
    "    # Add heat map layer\n",
    "    heat_data = [[lat, lon, score] for (lat, lon), score in zip(valid_locations, severity_scores) \n",
    "                if isinstance(lat, (int, float)) and isinstance(lon, (int, float))]\n",
    "    \n",
    "    if heat_data:\n",
    "        HeatMap(heat_data).add_to(m)\n",
    "    \n",
    "    # Save map to HTML file\n",
    "    map_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"disaster_map.html\")\n",
    "    m.save(map_file)\n",
    "\n",
    "    with open(map_file, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    display(HTML(html_content))\n",
    "    \n",
    "    logger.info(f\"Interactive map saved to {map_file}\")\n",
    "    \n",
    "    return map_file\n",
    "\n",
    "def save_to_excel(data, filename):\n",
    "    \"\"\"Save data to Excel file\"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(filename)), exist_ok=True)\n",
    "        \n",
    "        # Save to Excel\n",
    "        df.to_excel(filename, index=False)\n",
    "        logger.info(f\"Data successfully saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data to Excel: {e}\")\n",
    "        return False\n",
    "\n",
    "def perform_temporal_analysis(events):\n",
    "    \"\"\"Perform temporal analysis of disaster events\"\"\"\n",
    "    if not events:\n",
    "        logger.warning(\"No events for temporal analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(events)\n",
    "    \n",
    "    # Ensure datetime format\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
    "    \n",
    "    # Group by date and disaster type\n",
    "    daily_counts = df.groupby([pd.Grouper(key='datetime', freq='D'), 'disaster_type']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Create temporal visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot daily counts by disaster type\n",
    "    daily_counts.plot(kind='line', marker='o', ax=plt.gca())\n",
    "    \n",
    "    plt.title('Temporal Distribution of Disaster Events')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Events')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Disaster Type')\n",
    "    \n",
    "    # Format x-axis to show dates nicely\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Save the visualization\n",
    "    temporal_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"temporal_analysis.png\")\n",
    "    plt.savefig(temporal_file)\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Temporal analysis visualization saved to {temporal_file}\")\n",
    "    \n",
    "    return temporal_file\n",
    "\n",
    "def generate_word_cloud(texts, disaster_type=None):\n",
    "    \"\"\"Generate word cloud from texts\"\"\"\n",
    "    if not texts:\n",
    "        logger.warning(\"No texts for word cloud generation\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all texts\n",
    "    combined_text = ' '.join(texts)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = word_tokenize(combined_text.lower())\n",
    "    filtered_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        max_words=200,\n",
    "        contour_width=3,\n",
    "        contour_color='steelblue'\n",
    "    ).generate(' '.join(filtered_words))\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    title = f\"Word Cloud for {disaster_type} Disaster Texts\" if disaster_type else \"Word Cloud for Disaster Texts\"\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    # Save the word cloud\n",
    "    filename = f\"wordcloud_{disaster_type.lower()}.png\" if disaster_type else \"wordcloud.png\"\n",
    "    wordcloud_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)\n",
    "    plt.savefig(wordcloud_file)\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Word cloud saved to {wordcloud_file}\")\n",
    "    \n",
    "    return wordcloud_file\n",
    "\n",
    "def generate_disaster_report(events, output_file=None):\n",
    "    \"\"\"Generate a comprehensive report of disaster events\"\"\"\n",
    "    if not events:\n",
    "        logger.warning(\"No events to generate report\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame if not already\n",
    "    if not isinstance(events, pd.DataFrame):\n",
    "        events_df = pd.DataFrame(events)\n",
    "    else:\n",
    "        events_df = events.copy()\n",
    "    \n",
    "    # Group by disaster type\n",
    "    disaster_summary = events_df.groupby('disaster_type').agg({\n",
    "        'severity_score': ['mean', 'max', 'count'],\n",
    "        'deaths': 'sum',\n",
    "        'injured': 'sum',\n",
    "        'affected': 'sum',\n",
    "        'homeless': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    disaster_summary.columns = ['_'.join(col).strip('_') for col in disaster_summary.columns.values]\n",
    "    \n",
    "    # Sort by count and severity\n",
    "    disaster_summary = disaster_summary.sort_values(['severity_score_count', 'severity_score_max'], \n",
    "                                                   ascending=[False, False])\n",
    "    \n",
    "    # Generate report text\n",
    "    report = \"# Disaster Events Report\\n\\n\"\n",
    "    report += f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "    \n",
    "    report += \"## Summary\\n\\n\"\n",
    "    report += f\"Total events detected: {len(events_df)}\\n\"\n",
    "    report += f\"Disaster types detected: {len(disaster_summary)}\\n\"\n",
    "    report += f\"Total deaths reported: {events_df['deaths'].sum()}\\n\"\n",
    "    report += f\"Total injured reported: {events_df['injured'].sum()}\\n\\n\"\n",
    "    \n",
    "    report += \"## Disaster Types Breakdown\\n\\n\"\n",
    "    for _, row in disaster_summary.iterrows():\n",
    "        report += f\"### {row['disaster_type'].title()}\\n\\n\"\n",
    "        report += f\"- Events: {int(row['severity_score_count'])}\\n\"\n",
    "        report += f\"- Average severity: {row['severity_score_mean']:.2f}\\n\"\n",
    "        report += f\"- Maximum severity: {row['severity_score_max']:.2f}\\n\"\n",
    "        report += f\"- Deaths: {int(row['deaths'])}\\n\"\n",
    "        report += f\"- Injured: {int(row['injured'])}\\n\"\n",
    "        report += f\"- Affected: {int(row['affected'])}\\n\"\n",
    "        report += f\"- Homeless: {int(row['homeless'])}\\n\\n\"\n",
    "    \n",
    "    report += \"## Most Severe Events\\n\\n\"\n",
    "    most_severe = events_df.sort_values('severity_score', ascending=False).head(10)\n",
    "    for i, (_, event) in enumerate(most_severe.iterrows()):\n",
    "        report += f\"{i+1}. **{event['disaster_type'].title()} in {event['location']}**\\n\"\n",
    "        report += f\"   - Severity: {event['severity_score']:.2f}\\n\"\n",
    "        report += f\"   - Date/Time: {event['date']} {event['time']}\\n\"\n",
    "        report += f\"   - Impact: {int(event['deaths'])} deaths, {int(event['injured'])} injured\\n\"\n",
    "        report += f\"   - Coordinates: ({event['latitude']:.4f}, {event['longitude']:.4f})\\n\\n\"\n",
    "    \n",
    "    # Save report to file if specified\n",
    "    if output_file:\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(report)\n",
    "            logger.info(f\"Report saved to {output_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save report: {e}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "def visualize_disaster_distribution(events):\n",
    "    \"\"\"Create visualizations for disaster distribution\"\"\"\n",
    "    if not isinstance(events, pd.DataFrame):\n",
    "        events_df = pd.DataFrame(events)\n",
    "    else:\n",
    "        events_df = events.copy()\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Disaster types distribution\n",
    "    disaster_counts = events_df['disaster_type'].value_counts()\n",
    "    disaster_counts.plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "    axes[0, 0].set_title('Distribution of Disaster Types')\n",
    "    axes[0, 0].set_xlabel('Disaster Type')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Severity distribution\n",
    "    axes[0, 1].hist(events_df['severity_score'], bins=10, color='salmon')\n",
    "    axes[0, 1].set_title('Distribution of Severity Scores')\n",
    "    axes[0, 1].set_xlabel('Severity Score')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # 3. Impact variables\n",
    "    impact_vars = ['deaths', 'injured', 'affected', 'homeless']\n",
    "    impact_sums = [events_df[var].sum() for var in impact_vars]\n",
    "    axes[1, 0].bar(impact_vars, impact_sums, color=['darkred', 'orange', 'blue', 'green'])\n",
    "    axes[1, 0].set_title('Total Impact by Category')\n",
    "    axes[1, 0].set_xlabel('Impact Category')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    # 4. Temporal distribution (by day)\n",
    "    events_df['date_only'] = pd.to_datetime(events_df['date'])\n",
    "    daily_counts = events_df.groupby('date_only').size()\n",
    "    daily_counts.plot(kind='line', marker='o', ax=axes[1, 1], color='purple')\n",
    "    axes[1, 1].set_title('Daily Distribution of Events')\n",
    "    axes[1, 1].set_xlabel('Date')\n",
    "    axes[1, 1].set_ylabel('Number of Events')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    viz_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"disaster_visualization.png\")\n",
    "    plt.savefig(viz_file)\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Disaster distribution visualization saved to {viz_file}\")\n",
    "    \n",
    "    return viz_file\n",
    "\n",
    "def export_for_dashboard(events, output_file=None):\n",
    "    \"\"\"Export event data in a format suitable for dashboard visualization\"\"\"\n",
    "    if not isinstance(events, pd.DataFrame):\n",
    "        events_df = pd.DataFrame(events)\n",
    "    else:\n",
    "        events_df = events.copy()\n",
    "    \n",
    "    # Create a simplified structure for dashboard\n",
    "    dashboard_data = {\n",
    "        \"events\": events_df.to_dict('records'),\n",
    "        \"summary\": {\n",
    "            \"total_events\": len(events_df),\n",
    "            \"total_deaths\": int(events_df['deaths'].sum()),\n",
    "            \"total_injured\": int(events_df['injured'].sum()),\n",
    "            \"total_affected\": int(events_df['affected'].sum()),\n",
    "            \"total_homeless\": int(events_df['homeless'].sum()),\n",
    "            \"disaster_types\": events_df['disaster_type'].value_counts().to_dict(),\n",
    "            \"avg_severity\": float(events_df['severity_score'].mean()),\n",
    "            \"max_severity\": float(events_df['severity_score'].max()),\n",
    "            \"min_severity\": float(events_df['severity_score'].min()),\n",
    "        },\n",
    "        \"temporal\": {\n",
    "            \"dates\": events_df.groupby('date').size().to_dict(),\n",
    "        },\n",
    "        \"generated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save to JSON file if specified\n",
    "    if output_file:\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dashboard_data, f, indent=2, default=str)\n",
    "            logger.info(f\"Dashboard data exported to {output_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to export dashboard data: {e}\")\n",
    "    \n",
    "    return dashboard_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clustering_algorithms(data):\n",
    "    \"\"\"Compare different clustering algorithms and select the best one\"\"\"\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # K-means\n",
    "    kmeans_scores = []\n",
    "    max_clusters = min(10, len(data) - 1)  # Limit max clusters\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(scaled_data)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        try:\n",
    "            silhouette = silhouette_score(scaled_data, labels)\n",
    "        except:\n",
    "            silhouette = -1\n",
    "        \n",
    "        kmeans_scores.append({\n",
    "            'k': k,\n",
    "            'silhouette': silhouette,\n",
    "            'labels': labels\n",
    "        })\n",
    "    \n",
    "    best_kmeans = max(kmeans_scores, key=lambda x: x['silhouette'])\n",
    "    results['kmeans'] = best_kmeans\n",
    "    \n",
    "    # DBSCAN\n",
    "    eps_values = [0.3, 0.5, 0.7, 1.0]\n",
    "    min_samples_values = [3, 5, 10]\n",
    "    \n",
    "    dbscan_scores = []\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(scaled_data)\n",
    "            \n",
    "            # Skip if all points are noise\n",
    "            if len(set(labels)) <= 1:\n",
    "                continue\n",
    "            \n",
    "            # Calculate metrics\n",
    "            try:\n",
    "                silhouette = silhouette_score(scaled_data, labels)\n",
    "            except:\n",
    "                silhouette = -1\n",
    "            \n",
    "            dbscan_scores.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'silhouette': silhouette,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    if dbscan_scores:\n",
    "        best_dbscan = max(dbscan_scores, key=lambda x: x['silhouette'])\n",
    "        results['dbscan'] = best_dbscan\n",
    "    \n",
    "    # Hierarchical clustering\n",
    "    linkage_methods = ['ward', 'complete', 'average']\n",
    "    hierarchical_scores = []\n",
    "    \n",
    "    for linkage in linkage_methods:\n",
    "        for k in range(2, max_clusters + 1):\n",
    "            hierarchical = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "            labels = hierarchical.fit_predict(scaled_data)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            try:\n",
    "                silhouette = silhouette_score(scaled_data, labels)\n",
    "            except:\n",
    "                silhouette = -1\n",
    "            \n",
    "            hierarchical_scores.append({\n",
    "                'k': k,\n",
    "                'linkage': linkage,\n",
    "                'silhouette': silhouette,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    best_hierarchical = max(hierarchical_scores, key=lambda x: x['silhouette'])\n",
    "    results['hierarchical'] = best_hierarchical\n",
    "    \n",
    "    # HDBSCAN\n",
    "    try:\n",
    "        min_cluster_sizes = [3, 5, 10]\n",
    "        min_samples_values = [3, 5, 10]\n",
    "        \n",
    "        hdbscan_scores = []\n",
    "        for min_cluster_size in min_cluster_sizes:\n",
    "            for min_samples in min_samples_values:\n",
    "                clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "                labels = clusterer.fit_predict(scaled_data)\n",
    "                \n",
    "                # Skip if all points are noise\n",
    "                if len(set(labels)) <= 1:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate metrics\n",
    "                try:\n",
    "                    silhouette = silhouette_score(scaled_data, labels)\n",
    "                except:\n",
    "                    silhouette = -1\n",
    "                \n",
    "                hdbscan_scores.append({\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'min_samples': min_samples,\n",
    "                    'silhouette': silhouette,\n",
    "                    'labels': labels\n",
    "                })\n",
    "        \n",
    "        if hdbscan_scores:\n",
    "            best_hdbscan = max(hdbscan_scores, key=lambda x: x['silhouette'])\n",
    "            results['hdbscan'] = best_hdbscan\n",
    "    except Exception as e:\n",
    "        logger.error(f\"HDBSCAN failed: {e}\")\n",
    "    \n",
    "    # Find best algorithm\n",
    "    best_algorithm = None\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for algorithm, result in results.items():\n",
    "        if result['silhouette'] > best_score:\n",
    "            best_score = result['silhouette']\n",
    "            best_algorithm = algorithm\n",
    "    \n",
    "    return best_algorithm, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the disaster event extraction\"\"\"\n",
    "    # Time range for the last 6 hours\n",
    "    current_time = datetime.utcnow()\n",
    "    time_6_hours_ago = current_time - timedelta(hours=6)\n",
    "    \n",
    "    logger.info(f\"Starting disaster event extraction for events since {time_6_hours_ago}\")\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    all_articles = []\n",
    "    article_metadata = []\n",
    "    all_twitter_data = []\n",
    "    \n",
    "    # Setup Twitter API\n",
    "    twitter_api = setup_twitter_api()\n",
    "    twitter_enabled = True  # Flag to track if Twitter API is working\n",
    "    \n",
    "    # Fetch and process articles for each disaster type\n",
    "    for query in disaster_queries:\n",
    "        logger.info(f\"Processing disaster type: {query}\")\n",
    "        \n",
    "        # 1. Process Google News\n",
    "        rss_url = f\"https://news.google.com/rss/search?q={query}\"\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        for entry in feed.entries:\n",
    "            published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))\n",
    "            if published_time >= time_6_hours_ago:\n",
    "                url = entry.link\n",
    "                try:\n",
    "                    html_content = fetch_article_content(url)\n",
    "                    if html_content:\n",
    "                        article_text = parse_article_content(html_content)\n",
    "                        all_articles.append(article_text)\n",
    "                        article_metadata.append({\n",
    "                            'query': query,\n",
    "                            'url': url,\n",
    "                            'published_time': published_time\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to process article: {url} - {e}\")\n",
    "        \n",
    "        # 2. Process Twitter data only if enabled\n",
    "        if twitter_api and twitter_enabled:\n",
    "            try:\n",
    "                tweets = fetch_twitter_data(twitter_api, query, hours_ago=6)\n",
    "                processed_tweets = process_twitter_data(tweets)\n",
    "                all_twitter_data.extend(processed_tweets)\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Error fetching tweets for query {query}: {error_msg}\")\n",
    "                # If we get a 403 Forbidden error, disable Twitter for future queries\n",
    "                if \"403 Forbidden\" in error_msg:\n",
    "                    twitter_enabled = False\n",
    "                    logger.warning(\"Twitter API access is forbidden. Disabling Twitter data collection.\")\n",
    "    \n",
    "    # Check if we have any data to process\n",
    "    if not all_articles and not all_twitter_data:\n",
    "        logger.warning(\"No articles or tweets found for processing\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Collected {len(all_articles)} articles and {len(all_twitter_data)} tweets\")\n",
    "    \n",
    "    # Process news articles\n",
    "    news_event_data = []\n",
    "    \n",
    "    for i, article in enumerate(all_articles):\n",
    "        # Extract locations\n",
    "        locations = extract_locations(article)\n",
    "        \n",
    "        # If no locations found, try to extract from title\n",
    "        if not locations and 'title' in article_metadata[i]:\n",
    "            title_locations = extract_locations(article_metadata[i].get('title', ''))\n",
    "            if title_locations:\n",
    "                locations = title_locations\n",
    "        \n",
    "        # Skip articles with no locations\n",
    "        if not locations:\n",
    "            continue\n",
    "            \n",
    "        # Extract impact variables using BERT and regex\n",
    "        deaths, injured, affected, homeless = extract_impact_variables(article)\n",
    "        \n",
    "        # Calculate severity score\n",
    "        severity_score = continuous_severity_classification(deaths, injured, affected, homeless)\n",
    "        \n",
    "        # Extract dates\n",
    "        dates = extract_dates(article)\n",
    "        event_date = dates[0].strftime(\"%Y-%m-%d\") if dates else article_metadata[i]['published_time'].strftime(\"%Y-%m-%d\")\n",
    "        event_time = article_metadata[i]['published_time'].strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        sentiment = analyze_sentiment(article)\n",
    "        \n",
    "        # Store the disaster type\n",
    "        disaster_type = article_metadata[i]['query']\n",
    "        \n",
    "        # Get coordinates for each location\n",
    "        for location in locations:\n",
    "            coords = location_to_coordinates(location)\n",
    "            if coords and isinstance(coords, tuple):\n",
    "                news_event_data.append({\n",
    "                    'source': 'news',\n",
    "                    'disaster_type': disaster_type,\n",
    "                    'location': location,\n",
    "                    'latitude': coords[0],\n",
    "                    'longitude': coords[1],\n",
    "                    'date': event_date,\n",
    "                    'time': event_time,\n",
    "                    'severity_score': severity_score,\n",
    "                    'deaths': deaths,\n",
    "                    'injured': injured,\n",
    "                    'affected': affected,\n",
    "                    'homeless': homeless,\n",
    "                    'sentiment': sentiment['compound'],\n",
    "                    'url': article_metadata[i]['url']\n",
    "                })\n",
    "    \n",
    "    # Combine news and Twitter data\n",
    "    all_event_data = news_event_data + all_twitter_data\n",
    "    \n",
    "    if not all_event_data:\n",
    "        logger.warning(\"No event data extracted\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(all_event_data)\n",
    "    \n",
    "    # Remove duplicates based on location and disaster type\n",
    "    df = df.drop_duplicates(subset=['location', 'disaster_type'])\n",
    "    \n",
    "    # Save all events to Excel\n",
    "    excel_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"disaster_events.xlsx\")\n",
    "    save_to_excel(df.to_dict('records'), excel_file)\n",
    "    \n",
    "    # Save Twitter data separately if we have any\n",
    "    if all_twitter_data:\n",
    "        twitter_df = pd.DataFrame(all_twitter_data)\n",
    "        twitter_excel_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"twitter_disaster_events.xlsx\")\n",
    "        save_to_excel(twitter_df.to_dict('records'), twitter_excel_file)\n",
    "        \n",
    "        # Analyze Twitter trends\n",
    "        analyze_twitter_trends(twitter_df)\n",
    "    \n",
    "    # Continue with clustering and visualization as before\n",
    "    if len(df) >= 2:  # Need at least 2 points for clustering\n",
    "        # Prepare feature matrix for clustering\n",
    "        X = df[['latitude', 'longitude', 'severity_score']].values\n",
    "        \n",
    "        # Compare clustering algorithms\n",
    "        best_algorithm, clustering_results = compare_clustering_algorithms(X)\n",
    "        \n",
    "        logger.info(f\"Best clustering algorithm: {best_algorithm}\")\n",
    "        \n",
    "        # Get labels from best algorithm\n",
    "        labels = clustering_results[best_algorithm]['labels']\n",
    "        \n",
    "        # Add cluster labels to DataFrame\n",
    "        df['cluster'] = labels\n",
    "        \n",
    "        # Find most severe events in each cluster\n",
    "        severe_events = []\n",
    "        \n",
    "        for cluster_id in set(labels):\n",
    "            if cluster_id != -1:  # Skip noise points\n",
    "                cluster_df = df[df['cluster'] == cluster_id]\n",
    "                if not cluster_df.empty:\n",
    "                    most_severe = cluster_df.loc[cluster_df['severity_score'].idxmax()]\n",
    "                    severe_events.append(most_severe.to_dict())\n",
    "        \n",
    "        # Sort by severity score\n",
    "        severe_events = sorted(severe_events, key=lambda x: x['severity_score'], reverse=True)\n",
    "        \n",
    "        # Save severe events to Excel\n",
    "        severe_excel_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"severe_disaster_events.xlsx\")\n",
    "        save_to_excel(severe_events, severe_excel_file)\n",
    "        \n",
    "        # Create interactive map\n",
    "        map_file = create_interactive_map(\n",
    "            [(event['latitude'], event['longitude']) for event in severe_events],\n",
    "            [event['severity_score'] for event in severe_events],\n",
    "            [event['disaster_type'] for event in severe_events]\n",
    "        )\n",
    "        \n",
    "        # Display the map in the notebook\n",
    "        from IPython.display import display, HTML\n",
    "        try:\n",
    "            with open(map_file, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            display(HTML(html_content))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to display map in notebook: {e}\")\n",
    "        \n",
    "        # Perform temporal analysis\n",
    "        temporal_data = perform_temporal_analysis(df.to_dict('records'))\n",
    "        \n",
    "        # Make sure plots are displayed in the notebook\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        if isinstance(temporal_data, pd.Series):\n",
    "            temporal_data.plot(kind='line', marker='o')\n",
    "            plt.title('Temporal Distribution of Disaster Events')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Number of Events')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Generate word clouds\n",
    "        generate_word_cloud([article for article in all_articles])\n",
    "        plt.show()  # Make sure the plot is displayed\n",
    "        \n",
    "        # Generate word cloud for tweets if we have any\n",
    "        if all_twitter_data:\n",
    "            generate_word_cloud([tweet['text'] for tweet in all_twitter_data], disaster_type=\"Twitter\")\n",
    "            plt.show()  # Make sure the plot is displayed\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"disaster_report.md\")\n",
    "        generate_disaster_report(df, report_file)\n",
    "        \n",
    "        # Create visualizations\n",
    "        visualize_disaster_distribution(df)\n",
    "        plt.show()  # Make sure the plot is displayed\n",
    "        \n",
    "        # Export data for dashboard\n",
    "        dashboard_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"dashboard_data.json\")\n",
    "        export_for_dashboard(df, dashboard_file)\n",
    "        \n",
    "        # Print the most severe locations with their coordinates\n",
    "        print(\"\\nMost severe disaster events in the last 6 hours:\")\n",
    "        for i, event in enumerate(severe_events[:10]):  # Show top 10\n",
    "            source = event.get('source', 'unknown')\n",
    "            print(f\"{i+1}. [{source.upper()}] {event['disaster_type']} in {event['location']} - Severity: {event['severity_score']:.2f}\")\n",
    "            print(f\"   Coordinates: ({event['latitude']}, {event['longitude']})\")\n",
    "            print(f\"   Date/Time: {event['date']} {event['time']}\")\n",
    "            print(f\"   Impact: {event['deaths']} deaths, {event['injured']} injured\")\n",
    "            print()\n",
    "    else:\n",
    "        logger.warning(\"Not enough data points for clustering\")\n",
    "        \n",
    "        # Still create map with available data\n",
    "        if not df.empty:\n",
    "            map_file = create_interactive_map(\n",
    "                [(row['latitude'], row['longitude']) for _, row in df.iterrows()],\n",
    "                [row['severity_score'] for _, row in df.iterrows()],\n",
    "                [row['disaster_type'] for _, row in df.iterrows()]\n",
    "            )\n",
    "            \n",
    "            # Display the map in the notebook\n",
    "            from IPython.display import display, HTML\n",
    "            try:\n",
    "                with open(map_file, 'r', encoding='utf-8') as f:\n",
    "                    html_content = f.read()\n",
    "                display(HTML(html_content))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to display map in notebook: {e}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these lines at the top of your notebook, after your imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_google_news_data(query, hours_ago=6, max_articles=20):\n",
    "    \"\"\"Fetch disaster-related news from Google News RSS feed\"\"\"\n",
    "    articles = []\n",
    "    try:\n",
    "        # Format the RSS URL\n",
    "        rss_url = f\"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en\"\n",
    "        \n",
    "        # Parse the feed\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        # Calculate time range\n",
    "        current_time = datetime.utcnow()\n",
    "        time_ago = current_time - timedelta(hours=hours_ago)\n",
    "        \n",
    "        # Process each entry\n",
    "        for entry in feed.entries[:max_articles]:\n",
    "            try:\n",
    "                # Parse published time\n",
    "                published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))\n",
    "                \n",
    "                # Check if article is within time range\n",
    "                if published_time >= time_ago:\n",
    "                    article_data = {\n",
    "                        'title': entry.title,\n",
    "                        'link': entry.link,\n",
    "                        'published': published_time,\n",
    "                        'source': entry.source.title if hasattr(entry, 'source') else \"Google News\",\n",
    "                        'query': query\n",
    "                    }\n",
    "                    \n",
    "                    # Fetch and parse article content\n",
    "                    html_content = fetch_article_content(entry.link)\n",
    "                    if html_content:\n",
    "                        article_text = parse_article_content(html_content)\n",
    "                        article_data['text'] = article_text\n",
    "                        articles.append(article_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing news entry: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Fetched {len(articles)} articles for query: {query}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching Google News for query {query}: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def process_news_articles(articles):\n",
    "    \"\"\"Process news articles to extract disaster information\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Extract text\n",
    "            text = article['text']\n",
    "            \n",
    "            # Extract locations\n",
    "            locations = extract_locations(text)\n",
    "            \n",
    "            # If no locations found, try to extract from title\n",
    "            if not locations:\n",
    "                title_locations = extract_locations(article['title'])\n",
    "                if title_locations:\n",
    "                    locations = title_locations\n",
    "            \n",
    "            # Extract impact variables\n",
    "            deaths, injured, affected, homeless = extract_impact_variables(text)\n",
    "            \n",
    "            # Calculate severity score\n",
    "            severity_score = continuous_severity_classification(deaths, injured, affected, homeless)\n",
    "            \n",
    "            # Extract dates\n",
    "            dates = extract_dates(text)\n",
    "            event_date = dates[0].strftime(\"%Y-%m-%d\") if dates else article['published'].strftime(\"%Y-%m-%d\")\n",
    "            event_time = article['published'].strftime(\"%H:%M:%S\")\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            sentiment = analyze_sentiment(text)\n",
    "            \n",
    "            # Store the disaster type\n",
    "            disaster_type = article['query']\n",
    "            \n",
    "            # Verify disaster type using content\n",
    "            verified_type, confidence = classify_disaster_type(text)\n",
    "            if verified_type and confidence > 0.7:\n",
    "                disaster_type = verified_type\n",
    "            \n",
    "            # Get coordinates for each location\n",
    "            for location in locations:\n",
    "                coords = location_to_coordinates(location)\n",
    "                if coords and isinstance(coords, tuple):\n",
    "                    event_data = {\n",
    "                        'source': 'news',\n",
    "                        'disaster_type': disaster_type,\n",
    "                        'location': location,\n",
    "                        'latitude': coords[0],\n",
    "                        'longitude': coords[1],\n",
    "                        'date': event_date,\n",
    "                        'time': event_time,\n",
    "                        'severity_score': severity_score,\n",
    "                        'deaths': deaths,\n",
    "                        'injured': injured,\n",
    "                        'affected': affected,\n",
    "                        'homeless': homeless,\n",
    "                        'sentiment': sentiment['compound'],\n",
    "                        'url': article['link'],\n",
    "                        'title': article['title'],\n",
    "                        'news_source': article['source']\n",
    "                    }\n",
    "                    processed_data.append(event_data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing article {article.get('link', 'unknown')}: {e}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Update main function to use enhanced Google News extraction\n",
    "def main():\n",
    "    \"\"\"Main function to run the disaster event extraction\"\"\"\n",
    "    # Time range for the last 6 hours\n",
    "    current_time = datetime.utcnow()\n",
    "    time_6_hours_ago = current_time - timedelta(hours=6)\n",
    "    \n",
    "    logger.info(f\"Starting disaster event extraction for events since {time_6_hours_ago}\")\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    all_news_data = []\n",
    "    \n",
    "    # Fetch and process articles for each disaster type\n",
    "    for query in disaster_queries:\n",
    "        logger.info(f\"Processing disaster type: {query}\")\n",
    "        \n",
    "        # Process Google News with enhanced extraction\n",
    "        articles = fetch_google_news_data(query, hours_ago=12, max_articles=30)  # Increased time range and article count\n",
    "        processed_articles = process_news_articles(articles)\n",
    "        all_news_data.extend(processed_articles)\n",
    "    \n",
    "    if not all_news_data:\n",
    "        logger.warning(\"No news articles found for processing\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Collected {len(all_news_data)} processed news articles\")\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(all_news_data)\n",
    "    \n",
    "    # Remove duplicates based on location and disaster type\n",
    "    df = df.drop_duplicates(subset=['location', 'disaster_type'])\n",
    "    \n",
    "    # Save all events to Excel\n",
    "    excel_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"disaster_events.xlsx\")\n",
    "    save_to_excel(df.to_dict('records'), excel_file)\n",
    "    \n",
    "    # Continue with clustering and visualization\n",
    "    if len(df) >= 2:  # Need at least 2 points for clustering\n",
    "        # Prepare feature matrix for clustering\n",
    "        X = df[['latitude', 'longitude', 'severity_score']].values\n",
    "        \n",
    "        # Compare clustering algorithms\n",
    "        best_algorithm, clustering_results = compare_clustering_algorithms(X)\n",
    "        \n",
    "        logger.info(f\"Best clustering algorithm: {best_algorithm}\")\n",
    "        \n",
    "        # Get labels from best algorithm\n",
    "        labels = clustering_results[best_algorithm]['labels']\n",
    "        \n",
    "        # Add cluster labels to DataFrame\n",
    "        df['cluster'] = labels\n",
    "        \n",
    "        # Find most severe events in each cluster\n",
    "        severe_events = []\n",
    "        \n",
    "        for cluster_id in set(labels):\n",
    "            if cluster_id != -1:  # Skip noise points\n",
    "                cluster_df = df[df['cluster'] == cluster_id]\n",
    "                if not cluster_df.empty:\n",
    "                    most_severe = cluster_df.loc[cluster_df['severity_score'].idxmax()]\n",
    "                    severe_events.append(most_severe.to_dict())\n",
    "        \n",
    "        # Sort by severity score\n",
    "        severe_events = sorted(severe_events, key=lambda x: x['severity_score'], reverse=True)\n",
    "        \n",
    "        # Save severe events to Excel\n",
    "        severe_excel_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"severe_disaster_events.xlsx\")\n",
    "        save_to_excel(severe_events, severe_excel_file)\n",
    "        \n",
    "        # Create interactive map\n",
    "        map_file = create_interactive_map(\n",
    "            [(event['latitude'], event['longitude']) for event in severe_events],\n",
    "            [event['severity_score'] for event in severe_events],\n",
    "            [event['disaster_type'] for event in severe_events]\n",
    "        )\n",
    "        \n",
    "        # Perform temporal analysis\n",
    "        perform_temporal_analysis(df.to_dict('records'))\n",
    "        \n",
    "        # Generate word clouds\n",
    "        generate_word_cloud([article['text'] for article in articles if 'text' in article])\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"disaster_report.md\")\n",
    "        generate_disaster_report(df, report_file)\n",
    "        \n",
    "        # Create visualizations\n",
    "        visualize_disaster_distribution(df)\n",
    "        plt.ion() \n",
    "        # Export data for dashboard\n",
    "        dashboard_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"dashboard_data.json\")\n",
    "        export_for_dashboard(df, dashboard_file)\n",
    "        \n",
    "        # Print the most severe locations with their coordinates\n",
    "        print(\"\\nMost severe disaster events in the last 12 hours:\")\n",
    "        for i, event in enumerate(severe_events[:10]):  # Show top 10\n",
    "            print(f\"{i+1}. {event['disaster_type']} in {event['location']} - Severity: {event['severity_score']:.2f}\")\n",
    "            print(f\"   Coordinates: ({event['latitude']}, {event['longitude']})\")\n",
    "            print(f\"   Date/Time: {event['date']} {event['time']}\")\n",
    "            print(f\"   Impact: {event['deaths']} deaths, {event['injured']} injured\")\n",
    "            print(f\"   Source: {event.get('news_source', 'Unknown')}\")\n",
    "            print(f\"   URL: {event['url']}\")\n",
    "            print()\n",
    "    else:\n",
    "        logger.warning(\"Not enough data points for clustering\")\n",
    "        \n",
    "        # Still create map with available data\n",
    "        if not df.empty:\n",
    "            map_file = create_interactive_map(\n",
    "                [(row['latitude'], row['longitude']) for _, row in df.iterrows()],\n",
    "                [row['severity_score'] for _, row in df.iterrows()],\n",
    "                [row['disaster_type'] for _, row in df.iterrows()]\n",
    "            )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_news_sources(news_data):\n",
    "    \"\"\"Analyze distribution and reliability of news sources\"\"\"\n",
    "    if not news_data:\n",
    "        logger.warning(\"No news data to analyze\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame if not already\n",
    "    if not isinstance(news_data, pd.DataFrame):\n",
    "        df = pd.DataFrame(news_data)\n",
    "    else:\n",
    "        df = news_data.copy()\n",
    "    \n",
    "    # Count articles by source\n",
    "    source_counts = df['news_source'].value_counts()\n",
    "    \n",
    "    # Calculate average severity by source\n",
    "    severity_by_source = df.groupby('news_source')['severity_score'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot sources by article count\n",
    "    plt.subplot(2, 1, 1)\n",
    "    source_counts.head(15).plot(kind='bar', color='skyblue')\n",
    "    plt.title('Top News Sources by Article Count')\n",
    "    plt.xlabel('News Source')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Plot sources by average severity\n",
    "    plt.subplot(2, 1, 2)\n",
    "    severity_by_source.head(15).plot(kind='bar', color='salmon')\n",
    "    plt.title('Top News Sources by Average Severity Score')\n",
    "    plt.xlabel('News Source')\n",
    "    plt.ylabel('Average Severity Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the visualization\n",
    "    viz_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"news_source_analysis.png\")\n",
    "    plt.savefig(viz_file)\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"News source analysis visualization saved to {viz_file}\")\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    \n",
    "    # Create report\n",
    "    report = \"# News Source Analysis\\n\\n\"\n",
    "    report += f\"Analysis generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "    \n",
    "    report += \"## Summary\\n\\n\"\n",
    "    report += f\"Total news articles analyzed: {len(df)}\\n\"\n",
    "    report += f\"Number of unique news sources: {df['news_source'].nunique()}\\n\\n\"\n",
    "    \n",
    "    report += \"## Top News Sources by Article Count\\n\\n\"\n",
    "    for source, count in source_counts.head(15).items():\n",
    "        report += f\"- {source}: {count} articles\\n\"\n",
    "    \n",
    "    report += \"\\n## Top News Sources by Average Severity\\n\\n\"\n",
    "    for source, severity in severity_by_source.head(15).items():\n",
    "        report += f\"- {source}: {severity:.2f} average severity\\n\"\n",
    "    \n",
    "    # Save report\n",
    "    report_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"news_source_analysis.md\")\n",
    "    try:\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        logger.info(f\"News source analysis report saved to {report_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save news source analysis report: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'source_counts': source_counts.to_dict(),\n",
    "        'severity_by_source': severity_by_source.to_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
